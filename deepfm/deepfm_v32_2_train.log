nohup: ignoring input
INFO - 2020/10/22 14:33 -- 0:00:00 - model: DeepFM
                                     num_class: 2
                                     valid_split: 0.1
                                     num_dnn_layer: 4
                                     hidden_size: 128
                                     dropout_rate: 0.5
                                     embed_dim: 8
                                     device: GPU
                                     batch_size_train: 500000
                                     batch_size_dev: 100000
                                     num_epochs: 500
                                     num_patience: 50
                                     lr: 0.01
                                     lr_decay: 0.8
                                     lr_patience: 5
                                     lr_monitor: dev_auc
                                     optimizer: Adam
                                     init_method: xavier_uniform_
                                     random_seed: 0
                                     from_pretrained: False
                                     transfer_learning: False
                                     use_bn: True
                                     use_dnn: True
                                     use_fm: True
                                     fm_dense_cross: False
                                     lr_reduce: True
                                     EarlyStopping: True
                                     test_run: False
                                     shuffle_before_epoch: True
                                     torch_device: cuda
                                     result_dir: train_result/2020_1022_143316/
                                     checkpoint_save: train_result/2020_1022_143316/model_checkpoint.pkl
                                     model_save: train_result/2020_1022_143316/torch_model.pth
INFO - 2020/10/22 14:33 -- 0:00:00 - Task PID code: 75962
INFO - 2020/10/22 14:33 -- 0:00:00 - torch model and log file saved directory:
INFO - 2020/10/22 14:33 -- 0:00:00 - /data1/xuwen/code/deepfm_v32_seq/train_result/2020_1022_143316/
INFO - 2020/10/22 14:33 -- 0:00:00 - All used features in-order:
INFO - 2020/10/22 14:33 -- 0:00:00 - 1: geek_position
                                     2: geek_city
                                     3: geek_major
                                     4: boss_position
                                     5: boss_city
                                     6: geek_workyears
                                     7: geek_degree
                                     8: job_workyears
                                     9: job_degree
                                     10: boss_title_type
                                     11: el
                                     12: eh
                                     13: geek_paddf_rate_7d
                                     14: geek_success_times_7d
                                     15: jl
                                     16: jh
                                     17: boss_min_chat_tdiff
                                     18: job_min_active_tdiff
                                     19: job_paddf_rate_7d
                                     20: job_success_times_7d
                                     21: boss_paddf_success_times_2d
                                     22: boss_paddf_success_rate_2d
                                     23: boss_paddf_pchat_rate_2d
                                     24: boss_paddf_rate_2d
                                     25: job_pas_addf_num_24h
                                     26: job_paddf_success_times_2d
                                     27: job_paddf_success_times_7d
                                     28: job_paddf_rate_14d
                                     29: job_success_times_2d
                                     30: job_psuccess_times_7d
                                     31: job_paddfchat_times_7d
                                     32: job_type
INFO - 2020/10/22 14:33 -- 0:00:00 - Train data dir:/data1/xuwen/dataset/qm_geek_rank_success_train_v32/
INFO - 2020/10/22 14:33 -- 0:00:00 - Test data dir:/data1/xuwen/dataset/qm_geek_rank_success_test_v32/2020-10-20/
INFO - 2020/10/22 14:35 -- 0:02:00 -  Train df memory used: 4.7082 GB.
INFO - 2020/10/22 14:42 -- 0:09:16 - Sparse ID column indices for model: [0, 1, 2, 3, 4]
INFO - 2020/10/22 14:42 -- 0:09:16 - Sparse Non-ID column indices for model: [5, 6, 7, 8, 9]
INFO - 2020/10/22 14:42 -- 0:09:16 - Dense column indices for model: [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
INFO - 2020/10/22 14:42 -- 0:09:16 - max index of each ID type categorical feature: [1234, 397, 2815, 1234, 397]
INFO - 2020/10/22 14:42 -- 0:09:16 - max index of sequence features: 1234
INFO - 2020/10/22 14:42 -- 0:09:16 - Building batch iters...
INFO - 2020/10/22 14:42 -- 0:09:16 - Building model's network.
INFO - 2020/10/22 14:42 -- 0:09:24 - New model builded, save initilized checkpoint to the path: train_result/2020_1022_143316/model_checkpoint.pkl
INFO - 2020/10/22 14:42 -- 0:09:24 - save initilized model to the path: train_result/2020_1022_143316/torch_model.pth
INFO - 2020/10/22 14:42 -- 0:09:24 - start to train
INFO - 2020/10/22 14:42 -- 0:09:24 - number of train batch iters: 37
INFO - 2020/10/22 14:43 -- 0:10:41 - 0th epoch finished, train loss: 0.5690352, train auc: 0.5540736, val loss: 0.2269443, val auc: 0.5459495
INFO - 2020/10/22 14:45 -- 0:11:53 - 1th epoch finished, train loss: 0.5241832, train auc: 0.6226901, val loss: 0.3402095, val auc: 0.5483767
INFO - 2020/10/22 14:46 -- 0:13:07 - 2th epoch finished, train loss: 0.5180009, train auc: 0.6491359, val loss: 0.3671032, val auc: 0.5497951
INFO - 2020/10/22 14:47 -- 0:14:21 - 3th epoch finished, train loss: 0.5148771, train auc: 0.660835, val loss: 0.3430792, val auc: 0.5508518
INFO - 2020/10/22 14:48 -- 0:15:35 - 4th epoch finished, train loss: 0.5134764, train auc: 0.6628552, val loss: 0.3579494, val auc: 0.5525473
INFO - 2020/10/22 14:50 -- 0:16:50 - 5th epoch finished, train loss: 0.5122648, train auc: 0.6655453, val loss: 0.3984434, val auc: 0.5546327
INFO - 2020/10/22 14:51 -- 0:18:03 - 6th epoch finished, train loss: 0.5124256, train auc: 0.665439, val loss: 0.3323303, val auc: 0.5567056
INFO - 2020/10/22 14:52 -- 0:19:16 - 7th epoch finished, train loss: 0.5097866, train auc: 0.6712924, val loss: 0.2521832, val auc: 0.560762
INFO - 2020/10/22 14:53 -- 0:20:30 - 8th epoch finished, train loss: 0.5123069, train auc: 0.6648119, val loss: 0.413635, val auc: 0.5604855
INFO - 2020/10/22 14:55 -- 0:22:02 - 9th epoch finished, train loss: 0.5087282, train auc: 0.6729471, val loss: 0.3252092, val auc: 0.5634516
INFO - 2020/10/22 14:56 -- 0:23:25 - 10th epoch finished, train loss: 0.5097174, train auc: 0.6718329, val loss: 0.3279787, val auc: 0.5636935
INFO - 2020/10/22 14:57 -- 0:24:42 - 11th epoch finished, train loss: 0.5068087, train auc: 0.6780136, val loss: 0.3147044, val auc: 0.5683402
INFO - 2020/10/22 14:59 -- 0:25:57 - 12th epoch finished, train loss: 0.5049134, train auc: 0.6819219, val loss: 0.3605839, val auc: 0.5713168
INFO - 2020/10/22 15:00 -- 0:27:10 - 13th epoch finished, train loss: 0.5179612, train auc: 0.6489651, val loss: 0.3425725, val auc: 0.5639183
INFO - 2020/10/22 15:01 -- 0:28:23 - 14th epoch finished, train loss: 0.5070407, train auc: 0.6784932, val loss: 0.3607289, val auc: 0.5656795
INFO - 2020/10/22 15:02 -- 0:29:39 - 15th epoch finished, train loss: 0.5017899, train auc: 0.6885673, val loss: 0.4010838, val auc: 0.5727208
INFO - 2020/10/22 15:04 -- 0:30:55 - 16th epoch finished, train loss: 0.5079423, train auc: 0.6784153, val loss: 0.3889144, val auc: 0.5677358
INFO - 2020/10/22 15:05 -- 0:32:11 - 17th epoch finished, train loss: 0.5002473, train auc: 0.6960583, val loss: 0.3778109, val auc: 0.5851692
INFO - 2020/10/22 15:06 -- 0:33:25 - 18th epoch finished, train loss: 0.4971627, train auc: 0.7041784, val loss: 0.3529047, val auc: 0.5869169
INFO - 2020/10/22 15:07 -- 0:34:40 - 19th epoch finished, train loss: 0.4937942, train auc: 0.7118712, val loss: 0.3954889, val auc: 0.5890866
INFO - 2020/10/22 15:09 -- 0:35:55 - 20th epoch finished, train loss: 0.492525, train auc: 0.7147745, val loss: 0.3562185, val auc: 0.5896532
INFO - 2020/10/22 15:10 -- 0:37:09 - 21th epoch finished, train loss: 0.4926847, train auc: 0.7139592, val loss: 0.3588421, val auc: 0.5902299
INFO - 2020/10/22 15:11 -- 0:38:24 - 22th epoch finished, train loss: 0.4932834, train auc: 0.7126822, val loss: 0.364654, val auc: 0.5887587
INFO - 2020/10/22 15:12 -- 0:39:36 - 23th epoch finished, train loss: 0.4904166, train auc: 0.7188724, val loss: 0.3370866, val auc: 0.5905964
INFO - 2020/10/22 15:14 -- 0:40:51 - 24th epoch finished, train loss: 0.4918115, train auc: 0.7155509, val loss: 0.3871772, val auc: 0.5906596
INFO - 2020/10/22 15:15 -- 0:42:09 - 25th epoch finished, train loss: 0.489014, train auc: 0.7219268, val loss: 0.3879195, val auc: 0.5916308
INFO - 2020/10/22 15:16 -- 0:43:23 - 26th epoch finished, train loss: 0.488972, train auc: 0.7219144, val loss: 0.4404005, val auc: 0.5925242
INFO - 2020/10/22 15:17 -- 0:44:40 - 27th epoch finished, train loss: 0.4891236, train auc: 0.7213313, val loss: 0.4170559, val auc: 0.5918474
INFO - 2020/10/22 15:19 -- 0:45:56 - 28th epoch finished, train loss: 0.4893443, train auc: 0.7207646, val loss: 0.382359, val auc: 0.5908113
INFO - 2020/10/22 15:20 -- 0:47:11 - 29th epoch finished, train loss: 0.4881823, train auc: 0.7233691, val loss: 0.3712375, val auc: 0.592923
INFO - 2020/10/22 15:21 -- 0:48:26 - 30th epoch finished, train loss: 0.4876169, train auc: 0.7241444, val loss: 0.384252, val auc: 0.5934826
INFO - 2020/10/22 15:23 -- 0:49:44 - 31th epoch finished, train loss: 0.4860067, train auc: 0.7276894, val loss: 0.3746982, val auc: 0.5937468
INFO - 2020/10/22 15:24 -- 0:50:59 - 32th epoch finished, train loss: 0.4860427, train auc: 0.7274782, val loss: 0.3623249, val auc: 0.594113
INFO - 2020/10/22 15:25 -- 0:52:12 - 33th epoch finished, train loss: 0.4864337, train auc: 0.7266675, val loss: 0.3441987, val auc: 0.5937699
INFO - 2020/10/22 15:26 -- 0:53:27 - 34th epoch finished, train loss: 0.4857586, train auc: 0.7279536, val loss: 0.430451, val auc: 0.5947056
INFO - 2020/10/22 15:27 -- 0:54:41 - 35th epoch finished, train loss: 0.4862836, train auc: 0.7270062, val loss: 0.3551542, val auc: 0.5954462
INFO - 2020/10/22 15:29 -- 0:55:55 - 36th epoch finished, train loss: 0.4845453, train auc: 0.7301419, val loss: 0.3720945, val auc: 0.5953727
INFO - 2020/10/22 15:30 -- 0:57:12 - 37th epoch finished, train loss: 0.4833737, train auc: 0.7326944, val loss: 0.3886274, val auc: 0.5963515
INFO - 2020/10/22 15:31 -- 0:58:28 - 38th epoch finished, train loss: 0.4833073, train auc: 0.7324777, val loss: 0.4054707, val auc: 0.5967023
INFO - 2020/10/22 15:32 -- 0:59:43 - 39th epoch finished, train loss: 0.483533, train auc: 0.7320977, val loss: 0.40242, val auc: 0.5970332
INFO - 2020/10/22 15:34 -- 1:01:03 - 40th epoch finished, train loss: 0.4825209, train auc: 0.7342553, val loss: 0.4187983, val auc: 0.5978049
INFO - 2020/10/22 15:35 -- 1:02:20 - 41th epoch finished, train loss: 0.4887004, train auc: 0.7220168, val loss: 0.3504441, val auc: 0.5960934
INFO - 2020/10/22 15:36 -- 1:03:34 - 42th epoch finished, train loss: 0.4822447, train auc: 0.7347253, val loss: 0.3973713, val auc: 0.5978917
INFO - 2020/10/22 15:38 -- 1:04:50 - 43th epoch finished, train loss: 0.4810303, train auc: 0.7371014, val loss: 0.3885709, val auc: 0.5984537
INFO - 2020/10/22 15:39 -- 1:06:04 - 44th epoch finished, train loss: 0.4804864, train auc: 0.7380625, val loss: 0.3834808, val auc: 0.5988582
INFO - 2020/10/22 15:40 -- 1:07:18 - 45th epoch finished, train loss: 0.4800274, train auc: 0.7388647, val loss: 0.3737111, val auc: 0.5996818
INFO - 2020/10/22 15:41 -- 1:08:31 - 46th epoch finished, train loss: 0.4805126, train auc: 0.7377749, val loss: 0.378761, val auc: 0.6001758
INFO - 2020/10/22 15:43 -- 1:09:45 - 47th epoch finished, train loss: 0.4802282, train auc: 0.7382061, val loss: 0.4251672, val auc: 0.6004129
INFO - 2020/10/22 15:44 -- 1:10:59 - 48th epoch finished, train loss: 0.4799992, train auc: 0.7389806, val loss: 0.4406107, val auc: 0.6012524
INFO - 2020/10/22 15:45 -- 1:12:15 - 49th epoch finished, train loss: 0.4788334, train auc: 0.7410295, val loss: 0.4074408, val auc: 0.6017595
INFO - 2020/10/22 15:46 -- 1:13:30 - 50th epoch finished, train loss: 0.4780108, train auc: 0.7425036, val loss: 0.4214795, val auc: 0.6023791
INFO - 2020/10/22 15:48 -- 1:14:45 - 51th epoch finished, train loss: 0.477996, train auc: 0.7424358, val loss: 0.3913867, val auc: 0.603039
INFO - 2020/10/22 15:49 -- 1:15:59 - 52th epoch finished, train loss: 0.4777079, train auc: 0.7430062, val loss: 0.3425739, val auc: 0.6025796
INFO - 2020/10/22 15:50 -- 1:17:13 - 53th epoch finished, train loss: 0.4777432, train auc: 0.7428456, val loss: 0.3996484, val auc: 0.6040696
INFO - 2020/10/22 15:51 -- 1:18:28 - 54th epoch finished, train loss: 0.4780767, train auc: 0.7421233, val loss: 0.3815073, val auc: 0.6048197
INFO - 2020/10/22 15:53 -- 1:19:44 - 55th epoch finished, train loss: 0.4759384, train auc: 0.7459781, val loss: 0.3740698, val auc: 0.6051755
INFO - 2020/10/22 15:54 -- 1:21:04 - 56th epoch finished, train loss: 0.4754718, train auc: 0.7468882, val loss: 0.3864399, val auc: 0.6057994
INFO - 2020/10/22 15:55 -- 1:22:23 - 57th epoch finished, train loss: 0.4749474, train auc: 0.7477534, val loss: 0.3880787, val auc: 0.6064953
INFO - 2020/10/22 15:56 -- 1:23:40 - 58th epoch finished, train loss: 0.4752421, train auc: 0.7469869, val loss: 0.3894923, val auc: 0.6073569
INFO - 2020/10/22 15:58 -- 1:24:53 - 59th epoch finished, train loss: 0.4748802, train auc: 0.7477581, val loss: 0.4012258, val auc: 0.6079551
INFO - 2020/10/22 15:59 -- 1:26:05 - 60th epoch finished, train loss: 0.4743631, train auc: 0.748741, val loss: 0.3490514, val auc: 0.6078027
INFO - 2020/10/22 16:00 -- 1:27:22 - 61th epoch finished, train loss: 0.4739193, train auc: 0.7495095, val loss: 0.3978003, val auc: 0.6085816
INFO - 2020/10/22 16:01 -- 1:28:40 - 62th epoch finished, train loss: 0.4731312, train auc: 0.7509213, val loss: 0.4104966, val auc: 0.6095353
INFO - 2020/10/22 16:03 -- 1:30:08 - 63th epoch finished, train loss: 0.4727593, train auc: 0.7513483, val loss: 0.394377, val auc: 0.6097327
INFO - 2020/10/22 16:04 -- 1:31:25 - 64th epoch finished, train loss: 0.4724871, train auc: 0.7518313, val loss: 0.3776144, val auc: 0.6102861
INFO - 2020/10/22 16:05 -- 1:32:39 - 65th epoch finished, train loss: 0.4724935, train auc: 0.7519778, val loss: 0.4403341, val auc: 0.6107599
INFO - 2020/10/22 16:07 -- 1:33:54 - 66th epoch finished, train loss: 0.4729515, train auc: 0.7508482, val loss: 0.3671656, val auc: 0.6107753
INFO - 2020/10/22 16:08 -- 1:35:09 - 67th epoch finished, train loss: 0.4715994, train auc: 0.7533863, val loss: 0.4076885, val auc: 0.6120849
INFO - 2020/10/22 16:09 -- 1:36:25 - 68th epoch finished, train loss: 0.4712495, train auc: 0.7539787, val loss: 0.4013231, val auc: 0.6122994
INFO - 2020/10/22 16:10 -- 1:37:41 - 69th epoch finished, train loss: 0.4707935, train auc: 0.7545915, val loss: 0.3887683, val auc: 0.6123604
INFO - 2020/10/22 16:12 -- 1:38:55 - 70th epoch finished, train loss: 0.4706038, train auc: 0.7550804, val loss: 0.4067474, val auc: 0.6134405
INFO - 2020/10/22 16:13 -- 1:40:11 - 71th epoch finished, train loss: 0.4707669, train auc: 0.7543096, val loss: 0.3964944, val auc: 0.6138426
INFO - 2020/10/22 16:14 -- 1:41:25 - 72th epoch finished, train loss: 0.4701732, train auc: 0.755675, val loss: 0.4020589, val auc: 0.6142973
INFO - 2020/10/22 16:15 -- 1:42:42 - 73th epoch finished, train loss: 0.4698947, train auc: 0.7563162, val loss: 0.4109128, val auc: 0.6145489
INFO - 2020/10/22 16:17 -- 1:43:56 - 74th epoch finished, train loss: 0.4696601, train auc: 0.7563395, val loss: 0.403796, val auc: 0.6148501
INFO - 2020/10/22 16:18 -- 1:45:08 - 75th epoch finished, train loss: 0.4693568, train auc: 0.7569531, val loss: 0.4004501, val auc: 0.61523
INFO - 2020/10/22 16:19 -- 1:46:22 - 76th epoch finished, train loss: 0.4692469, train auc: 0.7571775, val loss: 0.3903908, val auc: 0.61559
INFO - 2020/10/22 16:20 -- 1:47:35 - 77th epoch finished, train loss: 0.4690026, train auc: 0.7573796, val loss: 0.3966912, val auc: 0.6160602
INFO - 2020/10/22 16:22 -- 1:48:50 - 78th epoch finished, train loss: 0.469, train auc: 0.7574991, val loss: 0.4251449, val auc: 0.616388
INFO - 2020/10/22 16:23 -- 1:50:03 - 79th epoch finished, train loss: 0.4689332, train auc: 0.7575789, val loss: 0.3986952, val auc: 0.6164819
INFO - 2020/10/22 16:24 -- 1:51:16 - 80th epoch finished, train loss: 0.4683291, train auc: 0.7583873, val loss: 0.3838608, val auc: 0.617019
INFO - 2020/10/22 16:25 -- 1:52:28 - 81th epoch finished, train loss: 0.4681675, train auc: 0.7587015, val loss: 0.3803929, val auc: 0.6170733
INFO - 2020/10/22 16:26 -- 1:53:40 - 82th epoch finished, train loss: 0.4681195, train auc: 0.758768, val loss: 0.3926938, val auc: 0.6176709
INFO - 2020/10/22 16:28 -- 1:54:54 - 83th epoch finished, train loss: 0.4678965, train auc: 0.7590558, val loss: 0.3804081, val auc: 0.6177484
INFO - 2020/10/22 16:29 -- 1:56:06 - 84th epoch finished, train loss: 0.468052, train auc: 0.7589371, val loss: 0.4027075, val auc: 0.6185671
INFO - 2020/10/22 16:30 -- 1:57:19 - 85th epoch finished, train loss: 0.4676565, train auc: 0.7594814, val loss: 0.3908359, val auc: 0.6182054
INFO - 2020/10/22 16:31 -- 1:58:33 - 86th epoch finished, train loss: 0.4674319, train auc: 0.7598386, val loss: 0.3956686, val auc: 0.6188668
INFO - 2020/10/22 16:33 -- 1:59:47 - 87th epoch finished, train loss: 0.4672982, train auc: 0.7600232, val loss: 0.3870246, val auc: 0.6191316
INFO - 2020/10/22 16:34 -- 2:01:02 - 88th epoch finished, train loss: 0.4672665, train auc: 0.7601408, val loss: 0.4032323, val auc: 0.6192227
INFO - 2020/10/22 16:35 -- 2:02:18 - 89th epoch finished, train loss: 0.4671431, train auc: 0.7602206, val loss: 0.3823957, val auc: 0.6192008
INFO - 2020/10/22 16:36 -- 2:03:33 - 90th epoch finished, train loss: 0.4670446, train auc: 0.7604906, val loss: 0.3994923, val auc: 0.6196919
INFO - 2020/10/22 16:38 -- 2:04:46 - 91th epoch finished, train loss: 0.4668283, train auc: 0.7607236, val loss: 0.3897047, val auc: 0.6196124
INFO - 2020/10/22 16:39 -- 2:06:00 - 92th epoch finished, train loss: 0.4668308, train auc: 0.7607527, val loss: 0.3930412, val auc: 0.6199537
INFO - 2020/10/22 16:40 -- 2:07:15 - 93th epoch finished, train loss: 0.4667345, train auc: 0.7608639, val loss: 0.3841009, val auc: 0.6199617
INFO - 2020/10/22 16:41 -- 2:08:28 - 94th epoch finished, train loss: 0.4666396, train auc: 0.7610606, val loss: 0.3943433, val auc: 0.6203929
INFO - 2020/10/22 16:42 -- 2:09:42 - 95th epoch finished, train loss: 0.4665651, train auc: 0.7612131, val loss: 0.3857381, val auc: 0.6200039
INFO - 2020/10/22 16:44 -- 2:10:55 - 96th epoch finished, train loss: 0.4663701, train auc: 0.761331, val loss: 0.3912017, val auc: 0.6206629
INFO - 2020/10/22 16:45 -- 2:12:09 - 97th epoch finished, train loss: 0.4662645, train auc: 0.7615458, val loss: 0.3906183, val auc: 0.6207195
INFO - 2020/10/22 16:46 -- 2:13:23 - 98th epoch finished, train loss: 0.4660864, train auc: 0.7616623, val loss: 0.3852302, val auc: 0.6207781
INFO - 2020/10/22 16:47 -- 2:14:37 - 99th epoch finished, train loss: 0.4662425, train auc: 0.7616928, val loss: 0.4033814, val auc: 0.6211272
INFO - 2020/10/22 16:49 -- 2:15:49 - 100th epoch finished, train loss: 0.4660228, train auc: 0.7618472, val loss: 0.3901129, val auc: 0.6213313
INFO - 2020/10/22 16:50 -- 2:17:03 - 101th epoch finished, train loss: 0.4659855, train auc: 0.7619836, val loss: 0.3942414, val auc: 0.6211712
INFO - 2020/10/22 16:51 -- 2:18:14 - 102th epoch finished, train loss: 0.4659207, train auc: 0.7620825, val loss: 0.3948752, val auc: 0.6215499
INFO - 2020/10/22 16:52 -- 2:19:29 - 103th epoch finished, train loss: 0.465827, train auc: 0.7622237, val loss: 0.3900104, val auc: 0.6215613
INFO - 2020/10/22 16:53 -- 2:20:42 - 104th epoch finished, train loss: 0.4656872, train auc: 0.7623026, val loss: 0.3893421, val auc: 0.6214992
INFO - 2020/10/22 16:55 -- 2:21:54 - 105th epoch finished, train loss: 0.4657964, train auc: 0.7623621, val loss: 0.3942686, val auc: 0.6218405
INFO - 2020/10/22 16:56 -- 2:23:08 - 106th epoch finished, train loss: 0.4655296, train auc: 0.7624721, val loss: 0.3896254, val auc: 0.6219317
INFO - 2020/10/22 16:57 -- 2:24:21 - 107th epoch finished, train loss: 0.4656266, train auc: 0.7625471, val loss: 0.3916077, val auc: 0.6221444
INFO - 2020/10/22 16:58 -- 2:25:35 - 108th epoch finished, train loss: 0.4655611, train auc: 0.7626527, val loss: 0.3964031, val auc: 0.6223344
INFO - 2020/10/22 17:00 -- 2:26:48 - 109th epoch finished, train loss: 0.4655111, train auc: 0.7627207, val loss: 0.3932284, val auc: 0.6222997
INFO - 2020/10/22 17:01 -- 2:28:04 - 110th epoch finished, train loss: 0.4654743, train auc: 0.7628084, val loss: 0.3986368, val auc: 0.6222572
INFO - 2020/10/22 17:02 -- 2:29:20 - 111th epoch finished, train loss: 0.4654257, train auc: 0.7628262, val loss: 0.3890352, val auc: 0.6224219
INFO - 2020/10/22 17:03 -- 2:30:34 - 112th epoch finished, train loss: 0.4653856, train auc: 0.7629266, val loss: 0.3975601, val auc: 0.6226436
INFO - 2020/10/22 17:05 -- 2:31:48 - 113th epoch finished, train loss: 0.4652785, train auc: 0.7629777, val loss: 0.3891933, val auc: 0.6226397
INFO - 2020/10/22 17:06 -- 2:33:01 - 114th epoch finished, train loss: 0.4652275, train auc: 0.7630397, val loss: 0.3872985, val auc: 0.6226325
INFO - 2020/10/22 17:07 -- 2:34:14 - 115th epoch finished, train loss: 0.4651921, train auc: 0.7630896, val loss: 0.3910289, val auc: 0.6228427
INFO - 2020/10/22 17:08 -- 2:35:29 - 116th epoch finished, train loss: 0.465209, train auc: 0.7631884, val loss: 0.3961758, val auc: 0.6229208
INFO - 2020/10/22 17:10 -- 2:36:55 - 117th epoch finished, train loss: 0.4651629, train auc: 0.7632361, val loss: 0.3900101, val auc: 0.6229217
INFO - 2020/10/22 17:11 -- 2:38:11 - 118th epoch finished, train loss: 0.4651508, train auc: 0.7632926, val loss: 0.3973301, val auc: 0.62304
INFO - 2020/10/22 17:12 -- 2:39:24 - 119th epoch finished, train loss: 0.4649608, train auc: 0.7633466, val loss: 0.3847686, val auc: 0.6231836
INFO - 2020/10/22 17:13 -- 2:40:39 - 120th epoch finished, train loss: 0.4650906, train auc: 0.7633578, val loss: 0.3942533, val auc: 0.623336
INFO - 2020/10/22 17:15 -- 2:41:52 - 121th epoch finished, train loss: 0.4649933, train auc: 0.7634475, val loss: 0.3940081, val auc: 0.6232654
INFO - 2020/10/22 17:16 -- 2:43:07 - 122th epoch finished, train loss: 0.4649637, train auc: 0.763503, val loss: 0.3953376, val auc: 0.6232549
INFO - 2020/10/22 17:17 -- 2:44:26 - 123th epoch finished, train loss: 0.4648732, train auc: 0.7635407, val loss: 0.394325, val auc: 0.6233919
INFO - 2020/10/22 17:18 -- 2:45:41 - 124th epoch finished, train loss: 0.4649084, train auc: 0.763576, val loss: 0.3932846, val auc: 0.6233583
INFO - 2020/10/22 17:20 -- 2:46:53 - 125th epoch finished, train loss: 0.4648859, train auc: 0.7636184, val loss: 0.3976602, val auc: 0.6236258
INFO - 2020/10/22 17:21 -- 2:48:07 - 126th epoch finished, train loss: 0.4648554, train auc: 0.7636509, val loss: 0.3961868, val auc: 0.6233919
INFO - 2020/10/22 17:22 -- 2:49:20 - 127th epoch finished, train loss: 0.4646773, train auc: 0.7637145, val loss: 0.3860647, val auc: 0.6235227
INFO - 2020/10/22 17:23 -- 2:50:36 - 128th epoch finished, train loss: 0.4647908, train auc: 0.7637401, val loss: 0.3921348, val auc: 0.6236444
INFO - 2020/10/22 17:25 -- 2:51:55 - 129th epoch finished, train loss: 0.4647827, train auc: 0.7637811, val loss: 0.397345, val auc: 0.6237998
INFO - 2020/10/22 17:26 -- 2:53:08 - 130th epoch finished, train loss: 0.4647238, train auc: 0.7637974, val loss: 0.3871391, val auc: 0.62388
INFO - 2020/10/22 17:27 -- 2:54:22 - 131th epoch finished, train loss: 0.464802, train auc: 0.7638343, val loss: 0.3935155, val auc: 0.6238902
INFO - 2020/10/22 17:28 -- 2:55:38 - 132th epoch finished, train loss: 0.4646866, train auc: 0.7638816, val loss: 0.3945214, val auc: 0.6236837
INFO - 2020/10/22 17:30 -- 2:56:54 - 133th epoch finished, train loss: 0.4646554, train auc: 0.7639311, val loss: 0.3919955, val auc: 0.6238209
INFO - 2020/10/22 17:31 -- 2:58:08 - 134th epoch finished, train loss: 0.4645972, train auc: 0.7639504, val loss: 0.3894655, val auc: 0.6239213
INFO - 2020/10/22 17:32 -- 2:59:23 - 135th epoch finished, train loss: 0.4646047, train auc: 0.7639796, val loss: 0.3959848, val auc: 0.6239893
INFO - 2020/10/22 17:33 -- 3:00:38 - 136th epoch finished, train loss: 0.4645163, train auc: 0.7639963, val loss: 0.3899643, val auc: 0.6240058
INFO - 2020/10/22 17:35 -- 3:01:53 - 137th epoch finished, train loss: 0.4645921, train auc: 0.7640395, val loss: 0.3943034, val auc: 0.6240484
INFO - 2020/10/22 17:36 -- 3:03:13 - 138th epoch finished, train loss: 0.4645223, train auc: 0.7640674, val loss: 0.3902559, val auc: 0.6240758
INFO - 2020/10/22 17:37 -- 3:04:31 - 139th epoch finished, train loss: 0.4645101, train auc: 0.7640985, val loss: 0.3922674, val auc: 0.624229
INFO - 2020/10/22 17:39 -- 3:05:45 - 140th epoch finished, train loss: 0.4645884, train auc: 0.7641252, val loss: 0.394695, val auc: 0.6241032
INFO - 2020/10/22 17:40 -- 3:06:58 - 141th epoch finished, train loss: 0.4644306, train auc: 0.7641446, val loss: 0.3933788, val auc: 0.6241556
INFO - 2020/10/22 17:41 -- 3:08:12 - 142th epoch finished, train loss: 0.4644414, train auc: 0.7641674, val loss: 0.394093, val auc: 0.6241544
INFO - 2020/10/22 17:42 -- 3:09:26 - 143th epoch finished, train loss: 0.4643974, train auc: 0.7641805, val loss: 0.3928284, val auc: 0.6241519
INFO - 2020/10/22 17:43 -- 3:10:38 - 144th epoch finished, train loss: 0.4644174, train auc: 0.7642093, val loss: 0.3905886, val auc: 0.6242162
INFO - 2020/10/22 17:45 -- 3:11:51 - 145th epoch finished, train loss: 0.4644015, train auc: 0.7642352, val loss: 0.3899492, val auc: 0.6242677
INFO - 2020/10/22 17:46 -- 3:13:05 - 146th epoch finished, train loss: 0.4643235, train auc: 0.7642487, val loss: 0.3889831, val auc: 0.6243213
INFO - 2020/10/22 17:47 -- 3:14:22 - 147th epoch finished, train loss: 0.464401, train auc: 0.7642677, val loss: 0.3917797, val auc: 0.6243866
INFO - 2020/10/22 17:48 -- 3:15:35 - 148th epoch finished, train loss: 0.4643643, train auc: 0.7642849, val loss: 0.3920101, val auc: 0.6244064
INFO - 2020/10/22 17:50 -- 3:16:50 - 149th epoch finished, train loss: 0.4643584, train auc: 0.7643054, val loss: 0.3922746, val auc: 0.6243981
INFO - 2020/10/22 17:51 -- 3:18:06 - 150th epoch finished, train loss: 0.4643743, train auc: 0.7643229, val loss: 0.3925816, val auc: 0.6243254
INFO - 2020/10/22 17:52 -- 3:19:24 - 151th epoch finished, train loss: 0.4643497, train auc: 0.7643457, val loss: 0.3919823, val auc: 0.6244017
INFO - 2020/10/22 17:53 -- 3:20:41 - 152th epoch finished, train loss: 0.4643326, train auc: 0.7643557, val loss: 0.3931272, val auc: 0.6244911
INFO - 2020/10/22 17:55 -- 3:21:55 - 153th epoch finished, train loss: 0.4643213, train auc: 0.7643786, val loss: 0.3938883, val auc: 0.62436
INFO - 2020/10/22 17:56 -- 3:23:13 - 154th epoch finished, train loss: 0.4643435, train auc: 0.7643894, val loss: 0.3911257, val auc: 0.6243862
INFO - 2020/10/22 17:57 -- 3:24:27 - 155th epoch finished, train loss: 0.4642551, train auc: 0.7643889, val loss: 0.3919525, val auc: 0.6245166
INFO - 2020/10/22 17:59 -- 3:25:44 - 156th epoch finished, train loss: 0.4643011, train auc: 0.7644035, val loss: 0.3905746, val auc: 0.6245388
INFO - 2020/10/22 18:00 -- 3:26:58 - 157th epoch finished, train loss: 0.4642724, train auc: 0.7644321, val loss: 0.3917719, val auc: 0.624534
INFO - 2020/10/22 18:01 -- 3:28:19 - 158th epoch finished, train loss: 0.4642352, train auc: 0.7644468, val loss: 0.3930756, val auc: 0.6244228
INFO - 2020/10/22 18:02 -- 3:29:34 - 159th epoch finished, train loss: 0.4643187, train auc: 0.7644594, val loss: 0.3915361, val auc: 0.6245303
INFO - 2020/10/22 18:04 -- 3:30:55 - 160th epoch finished, train loss: 0.4642647, train auc: 0.7644659, val loss: 0.3922262, val auc: 0.6246705
INFO - 2020/10/22 18:05 -- 3:32:13 - 161th epoch finished, train loss: 0.4642176, train auc: 0.7644767, val loss: 0.3915271, val auc: 0.6246396
INFO - 2020/10/22 18:06 -- 3:33:30 - 162th epoch finished, train loss: 0.4643185, train auc: 0.7644968, val loss: 0.3953094, val auc: 0.6246563
INFO - 2020/10/22 18:08 -- 3:34:44 - 163th epoch finished, train loss: 0.4642304, train auc: 0.7645017, val loss: 0.3917614, val auc: 0.6247048
INFO - 2020/10/22 18:09 -- 3:36:02 - 164th epoch finished, train loss: 0.4642262, train auc: 0.7645133, val loss: 0.3917049, val auc: 0.6246412
INFO - 2020/10/22 18:10 -- 3:37:17 - 165th epoch finished, train loss: 0.4641641, train auc: 0.764522, val loss: 0.3924415, val auc: 0.6247183
INFO - 2020/10/22 18:11 -- 3:38:31 - 166th epoch finished, train loss: 0.4641013, train auc: 0.7645409, val loss: 0.3893094, val auc: 0.6246954
INFO - 2020/10/22 18:13 -- 3:39:45 - 167th epoch finished, train loss: 0.4641888, train auc: 0.7645456, val loss: 0.3925019, val auc: 0.6246395
INFO - 2020/10/22 18:14 -- 3:40:59 - 168th epoch finished, train loss: 0.4642505, train auc: 0.7645497, val loss: 0.3921848, val auc: 0.6246766
INFO - 2020/10/22 18:15 -- 3:42:15 - 169th epoch finished, train loss: 0.4642542, train auc: 0.7645682, val loss: 0.3921852, val auc: 0.6247208
INFO - 2020/10/22 18:16 -- 3:43:30 - 170th epoch finished, train loss: 0.4641429, train auc: 0.7645813, val loss: 0.3900843, val auc: 0.6247424
INFO - 2020/10/22 18:18 -- 3:44:53 - 171th epoch finished, train loss: 0.4641249, train auc: 0.7645825, val loss: 0.3905085, val auc: 0.6247529
INFO - 2020/10/22 18:19 -- 3:46:10 - 172th epoch finished, train loss: 0.4641725, train auc: 0.7645962, val loss: 0.3913386, val auc: 0.6247232
INFO - 2020/10/22 18:20 -- 3:47:23 - 173th epoch finished, train loss: 0.464063, train auc: 0.764604, val loss: 0.3914767, val auc: 0.6247161
INFO - 2020/10/22 18:21 -- 3:48:36 - 174th epoch finished, train loss: 0.4642765, train auc: 0.7646114, val loss: 0.3905466, val auc: 0.6247016
INFO - 2020/10/22 18:23 -- 3:49:48 - 175th epoch finished, train loss: 0.4641838, train auc: 0.7646225, val loss: 0.39278, val auc: 0.6247478
INFO - 2020/10/22 18:24 -- 3:51:02 - 176th epoch finished, train loss: 0.4641896, train auc: 0.7646242, val loss: 0.3941632, val auc: 0.6248007
INFO - 2020/10/22 18:25 -- 3:52:17 - 177th epoch finished, train loss: 0.4641351, train auc: 0.764632, val loss: 0.3920988, val auc: 0.6248085
INFO - 2020/10/22 18:26 -- 3:53:30 - 178th epoch finished, train loss: 0.464086, train auc: 0.7646389, val loss: 0.3914444, val auc: 0.6247782
INFO - 2020/10/22 18:28 -- 3:54:45 - 179th epoch finished, train loss: 0.4640282, train auc: 0.7646475, val loss: 0.3915089, val auc: 0.6247942
INFO - 2020/10/22 18:29 -- 3:55:59 - 180th epoch finished, train loss: 0.4641274, train auc: 0.7646406, val loss: 0.3927633, val auc: 0.6247757
INFO - 2020/10/22 18:30 -- 3:57:13 - 181th epoch finished, train loss: 0.4641122, train auc: 0.7646589, val loss: 0.3919962, val auc: 0.6248091
INFO - 2020/10/22 18:31 -- 3:58:28 - 182th epoch finished, train loss: 0.4641017, train auc: 0.7646636, val loss: 0.3917992, val auc: 0.6248934
INFO - 2020/10/22 18:32 -- 3:59:42 - 183th epoch finished, train loss: 0.464043, train auc: 0.7646672, val loss: 0.3914845, val auc: 0.6249079
INFO - 2020/10/22 18:34 -- 4:00:57 - 184th epoch finished, train loss: 0.4640624, train auc: 0.7646772, val loss: 0.3930987, val auc: 0.6248401
INFO - 2020/10/22 18:35 -- 4:02:11 - 185th epoch finished, train loss: 0.4640869, train auc: 0.7646784, val loss: 0.3947113, val auc: 0.624834
INFO - 2020/10/22 18:36 -- 4:03:26 - 186th epoch finished, train loss: 0.4640685, train auc: 0.7646807, val loss: 0.3916077, val auc: 0.6248345
INFO - 2020/10/22 18:38 -- 4:04:47 - 187th epoch finished, train loss: 0.4640105, train auc: 0.7646925, val loss: 0.3904887, val auc: 0.6248353
INFO - 2020/10/22 18:39 -- 4:06:03 - 188th epoch finished, train loss: 0.464162, train auc: 0.7646941, val loss: 0.3927217, val auc: 0.6248513
INFO - 2020/10/22 18:40 -- 4:07:19 - 189th epoch finished, train loss: 0.464056, train auc: 0.7646974, val loss: 0.3914926, val auc: 0.6248638
INFO - 2020/10/22 18:41 -- 4:08:42 - 190th epoch finished, train loss: 0.4640002, train auc: 0.7647019, val loss: 0.3906561, val auc: 0.6249214
INFO - 2020/10/22 18:43 -- 4:10:05 - 191th epoch finished, train loss: 0.464083, train auc: 0.7647047, val loss: 0.3913147, val auc: 0.6248472
INFO - 2020/10/22 18:44 -- 4:11:18 - 192th epoch finished, train loss: 0.4639792, train auc: 0.7647137, val loss: 0.3917778, val auc: 0.6249063
INFO - 2020/10/22 18:45 -- 4:12:33 - 193th epoch finished, train loss: 0.464161, train auc: 0.76471, val loss: 0.3915858, val auc: 0.6249143
INFO - 2020/10/22 18:47 -- 4:13:47 - 194th epoch finished, train loss: 0.4640291, train auc: 0.764722, val loss: 0.3925827, val auc: 0.6248757
INFO - 2020/10/22 18:48 -- 4:15:01 - 195th epoch finished, train loss: 0.4640361, train auc: 0.7647224, val loss: 0.3917812, val auc: 0.62495
INFO - 2020/10/22 18:49 -- 4:16:14 - 196th epoch finished, train loss: 0.4640639, train auc: 0.7647293, val loss: 0.3926103, val auc: 0.6248793
INFO - 2020/10/22 18:50 -- 4:17:26 - 197th epoch finished, train loss: 0.4639669, train auc: 0.7647286, val loss: 0.3917595, val auc: 0.6249053
INFO - 2020/10/22 18:51 -- 4:18:40 - 198th epoch finished, train loss: 0.4639726, train auc: 0.7647289, val loss: 0.3914049, val auc: 0.624958
INFO - 2020/10/22 18:53 -- 4:19:55 - 199th epoch finished, train loss: 0.4640314, train auc: 0.764737, val loss: 0.3918675, val auc: 0.6249347
INFO - 2020/10/22 18:54 -- 4:21:09 - 200th epoch finished, train loss: 0.4640113, train auc: 0.7647376, val loss: 0.3906549, val auc: 0.6248986
INFO - 2020/10/22 18:54 -- 4:21:09 - ======================================================================
INFO - 2020/10/22 18:54 -- 4:21:09 - Early Stopping triggered after 201 epoches, calculating test accuracy...
INFO - 2020/10/22 18:54 -- 4:21:09 - Current meomory used before delete train and dev: 22.704365 GB
INFO - 2020/10/22 18:54 -- 4:21:10 - Current meomory after delete train and dev: 2.500683 GB
INFO - 2020/10/22 18:55 -- 4:22:28 - test loss:0.3913107, test auc: 0.6199746 .
INFO - 2020/10/22 18:55 -- 4:22:28 - Saving onnx model...
INFO - 2020/10/22 18:55 -- 4:22:30 - Final torch model and log file saved directory:
INFO - 2020/10/22 18:55 -- 4:22:30 - /data1/xuwen/code/deepfm_v32_seq/train_result/2020_1022_143316/
Epoch     7: reducing learning rate of group 0 to 8.0000e-03.
Epoch    13: reducing learning rate of group 0 to 6.4000e-03.
Epoch    19: reducing learning rate of group 0 to 5.1200e-03.
Epoch    25: reducing learning rate of group 0 to 4.0960e-03.
Epoch    31: reducing learning rate of group 0 to 3.2768e-03.
Epoch    37: reducing learning rate of group 0 to 2.6214e-03.
Epoch    43: reducing learning rate of group 0 to 2.0972e-03.
Epoch    49: reducing learning rate of group 0 to 1.6777e-03.
Epoch    55: reducing learning rate of group 0 to 1.3422e-03.
Epoch    61: reducing learning rate of group 0 to 1.0737e-03.
Epoch    67: reducing learning rate of group 0 to 8.5899e-04.
Epoch    73: reducing learning rate of group 0 to 6.8719e-04.
Epoch    79: reducing learning rate of group 0 to 5.4976e-04.
Epoch    85: reducing learning rate of group 0 to 4.3980e-04.
Epoch    91: reducing learning rate of group 0 to 3.5184e-04.
Epoch    97: reducing learning rate of group 0 to 2.8147e-04.
Epoch   103: reducing learning rate of group 0 to 2.2518e-04.
Epoch   109: reducing learning rate of group 0 to 1.8014e-04.
Epoch   115: reducing learning rate of group 0 to 1.4412e-04.
Epoch   121: reducing learning rate of group 0 to 1.1529e-04.
Epoch   127: reducing learning rate of group 0 to 9.2234e-05.
Epoch   133: reducing learning rate of group 0 to 7.3787e-05.
Epoch   139: reducing learning rate of group 0 to 5.9030e-05.
Epoch   145: reducing learning rate of group 0 to 4.7224e-05.
Epoch   151: reducing learning rate of group 0 to 3.7779e-05.
Epoch   157: reducing learning rate of group 0 to 3.0223e-05.
Epoch   163: reducing learning rate of group 0 to 2.4179e-05.
Epoch   169: reducing learning rate of group 0 to 1.9343e-05.
Epoch   175: reducing learning rate of group 0 to 1.5474e-05.
Epoch   181: reducing learning rate of group 0 to 1.2379e-05.
Epoch   187: reducing learning rate of group 0 to 9.9035e-06.
Epoch   193: reducing learning rate of group 0 to 7.9228e-06.
Epoch   199: reducing learning rate of group 0 to 6.3383e-06.
graph(%input : Double(100000:54, 54:1),
      %1.embedding_cols.0.weight : Float(1234:8, 8:1),
      %1.embedding_cols.1.weight : Float(397:8, 8:1),
      %1.embedding_cols.2.weight : Float(2815:8, 8:1),
      %1.embedding_cols.3.weight : Float(1234:8, 8:1),
      %1.embedding_cols.4.weight : Float(397:8, 8:1),
      %1.embedding_seq_detail.weight : Float(1234:8, 8:1),
      %1.embedding_seq_addf.weight : Float(1234:8, 8:1),
      %1.embed_FM_1d_sparse_col.0.weight : Float(1234:1, 1:1),
      %1.embed_FM_1d_sparse_col.1.weight : Float(397:1, 1:1),
      %1.embed_FM_1d_sparse_col.2.weight : Float(2815:1, 1:1),
      %1.embed_FM_1d_sparse_col.3.weight : Float(1234:1, 1:1),
      %1.embed_FM_1d_sparse_col.4.weight : Float(397:1, 1:1),
      %1.embed_FM_1d_detail.weight : Float(1234:1, 1:1),
      %1.embed_FM_1d_addf.weight : Float(1234:1, 1:1),
      %1.fm_linear.weight : Float(27:1, 1:1),
      %1.fm_bn.weight : Float(1:1),
      %1.fm_bn.bias : Float(1:1),
      %1.fm_bn.running_mean : Float(1:1),
      %1.fm_bn.running_var : Float(1:1),
      %1.dnn.linear_layers.0.weight : Float(128:83, 83:1),
      %1.dnn.linear_layers.0.bias : Float(128:1),
      %1.dnn.linear_layers.1.weight : Float(64:128, 128:1),
      %1.dnn.linear_layers.1.bias : Float(64:1),
      %1.dnn.linear_layers.2.weight : Float(32:64, 64:1),
      %1.dnn.linear_layers.2.bias : Float(32:1),
      %1.dnn.linear_layers.3.weight : Float(16:32, 32:1),
      %1.dnn.linear_layers.3.bias : Float(16:1),
      %1.dnn.bn.0.weight : Float(128:1),
      %1.dnn.bn.0.bias : Float(128:1),
      %1.dnn.bn.0.running_mean : Float(128:1),
      %1.dnn.bn.0.running_var : Float(128:1),
      %1.dnn.bn.1.weight : Float(64:1),
      %1.dnn.bn.1.bias : Float(64:1),
      %1.dnn.bn.1.running_mean : Float(64:1),
      %1.dnn.bn.1.running_var : Float(64:1),
      %1.dnn.bn.2.weight : Float(32:1),
      %1.dnn.bn.2.bias : Float(32:1),
      %1.dnn.bn.2.running_mean : Float(32:1),
      %1.dnn.bn.2.running_var : Float(32:1),
      %1.dnn.bn.3.weight : Float(16:1),
      %1.dnn.bn.3.bias : Float(16:1),
      %1.dnn.bn.3.running_mean : Float(16:1),
      %1.dnn.bn.3.running_var : Float(16:1),
      %1.dnn_linear.weight : Float(1:16, 16:1),
      %1.dnn_linear.bias : Float(1:1),
      %177 : Float(),
      %178 : Long(1:1)):
  %53 : Tensor = onnx::Shape(%input)
  %54 : Tensor = onnx::Constant[value={0}]()
  %55 : Long() = onnx::Gather[axis=0](%53, %54) # /data1/xuwen/code/deepfm_v32_seq/models/DeepFM.py:72:0
  %56 : Tensor = onnx::Constant[value={0}]()
  %57 : Double(100000:54) = onnx::Gather[axis=1](%input, %56) # /data1/xuwen/code/deepfm_v32_seq/models/DeepFM.py:75:0
  %58 : Double(100000:54, 1:1) = onnx::Unsqueeze[axes=[1]](%57) # /data1/xuwen/code/deepfm_v32_seq/models/DeepFM.py:75:0
  %59 : Long(100000:1, 1:1) = onnx::Cast[to=7](%58) # /data1/xuwen/code/deepfm_v32_seq/models/DeepFM.py:75:0
  %60 : Float(100000:8, 1:8, 8:1) = onnx::Gather(%1.embedding_cols.0.weight, %59) # /usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1814:0
  %61 : Tensor = onnx::Constant[value={1}]()
  %62 : Double(100000:54) = onnx::Gather[axis=1](%input, %61) # /data1/xuwen/code/deepfm_v32_seq/models/DeepFM.py:75:0
  %63 : Double(100000:54, 1:1) = onnx::Unsqueeze[axes=[1]](%62) # /data1/xuwen/code/deepfm_v32_seq/models/DeepFM.py:75:0
  %64 : Long(100000:1, 1:1) = onnx::Cast[to=7](%63) # /data1/xuwen/code/deepfm_v32_seq/models/DeepFM.py:75:0
  %65 : Float(100000:8, 1:8, 8:1) = onnx::Gather(%1.embedding_cols.1.weight, %64) # /usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1814:0
  %66 : Tensor = onnx::Constant[value={2}]()
  %67 : Double(100000:54) = onnx::Gather[axis=1](%input, %66) # /data1/xuwen/code/deepfm_v32_seq/models/DeepFM.py:75:0
  %68 : Double(100000:54, 1:1) = onnx::Unsqueeze[axes=[1]](%67) # /data1/xuwen/code/deepfm_v32_seq/models/DeepFM.py:75:0
  %69 : Long(100000:1, 1:1) = onnx::Cast[to=7](%68) # /data1/xuwen/code/deepfm_v32_seq/models/DeepFM.py:75:0
  %70 : Float(100000:8, 1:8, 8:1) = onnx::Gather(%1.embedding_cols.2.weight, %69) # /usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1814:0
  %71 : Tensor = onnx::Constant[value={3}]()
  %72 : Double(100000:54) = onnx::Gather[axis=1](%input, %71) # /data1/xuwen/code/deepfm_v32_seq/models/DeepFM.py:75:0
  %73 : Double(100000:54, 1:1) = onnx::Unsqueeze[axes=[1]](%72) # /data1/xuwen/code/deepfm_v32_seq/models/DeepFM.py:75:0
  %74 : Long(100000:1, 1:1) = onnx::Cast[to=7](%73) # /data1/xuwen/code/deepfm_v32_seq/models/DeepFM.py:75:0
  %75 : Float(100000:8, 1:8, 8:1) = onnx::Gather(%1.embedding_cols.3.weight, %74) # /usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1814:0
  %76 : Tensor = onnx::Constant[value={4}]()
  %77 : Double(100000:54) = onnx::Gather[axis=1](%input, %76) # /data1/xuwen/code/deepfm_v32_seq/models/DeepFM.py:75:0
  %78 : Double(100000:54, 1:1) = onnx::Unsqueeze[axes=[1]](%77) # /data1/xuwen/code/deepfm_v32_seq/models/DeepFM.py:75:0
  %79 : Long(100000:1, 1:1) = onnx::Cast[to=7](%78) # /data1/xuwen/code/deepfm_v32_seq/models/DeepFM.py:75:0
  %80 : Float(100000:8, 1:8, 8:1) = onnx::Gather(%1.embedding_cols.4.weight, %79) # /usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1814:0
  %81 : Tensor = onnx::Constant[value={52}]()
  %82 : Double(100000:54) = onnx::Gather[axis=1](%input, %81) # /data1/xuwen/code/deepfm_v32_seq/models/DeepFM.py:77:0
  %83 : Double(100000:54, 1:1) = onnx::Unsqueeze[axes=[1]](%82) # /data1/xuwen/code/deepfm_v32_seq/models/DeepFM.py:77:0
  %84 : Long(10:1) = onnx::Constant[value= 32  33  34  35  36  37  38  39  40  41 [ CUDALongType{10} ]]()
  %85 : Double(100000:10, 10:1) = onnx::Gather[axis=1](%input, %84) # /data1/xuwen/code/deepfm_v32_seq/models/DeepFM.py:78:0
  %86 : Long(100000:10, 10:1) = onnx::Cast[to=7](%85) # /data1/xuwen/code/deepfm_v32_seq/models/DeepFM.py:78:0
  %87 : Float(100000:80, 10:8, 8:1) = onnx::Gather(%1.embedding_seq_detail.weight, %86) # /usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1814:0
  %88 : Float(100000:8, 8:1) = onnx::ReduceSum[axes=[1], keepdims=0](%87) # /data1/xuwen/code/deepfm_v32_seq/models/DeepFM.py:79:0
  %89 : Double(100000:8, 8:1) = onnx::Cast[to=11](%88)
  %90 : Double(100000:8, 8:1) = onnx::Div(%89, %83) # /data1/xuwen/code/deepfm_v32_seq/models/DeepFM.py:80:0
  %91 : Double(100000:8, 1:8, 8:1) = onnx::Unsqueeze[axes=[1]](%90) # /data1/xuwen/code/deepfm_v32_seq/models/DeepFM.py:81:0
  %92 : Float(100000:8, 1:8, 8:1) = onnx::Cast[to=1](%91) # /data1/xuwen/code/deepfm_v32_seq/models/DeepFM.py:81:0
  %93 : Tensor = onnx::Constant[value={53}]()
  %94 : Double(100000:54) = onnx::Gather[axis=1](%input, %93) # /data1/xuwen/code/deepfm_v32_seq/models/DeepFM.py:84:0
  %95 : Double(100000:54, 1:1) = onnx::Unsqueeze[axes=[1]](%94) # /data1/xuwen/code/deepfm_v32_seq/models/DeepFM.py:84:0
  %96 : Long(10:1) = onnx::Constant[value= 42  43  44  45  46  47  48  49  50  51 [ CUDALongType{10} ]]()
  %97 : Double(100000:10, 10:1) = onnx::Gather[axis=1](%input, %96) # /data1/xuwen/code/deepfm_v32_seq/models/DeepFM.py:85:0
  %98 : Long(100000:10, 10:1) = onnx::Cast[to=7](%97) # /data1/xuwen/code/deepfm_v32_seq/models/DeepFM.py:85:0
  %99 : Float(100000:80, 10:8, 8:1) = onnx::Gather(%1.embedding_seq_addf.weight, %98) # /usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1814:0
  %100 : Float(100000:8, 8:1) = onnx::ReduceSum[axes=[1], keepdims=0](%99) # /data1/xuwen/code/deepfm_v32_seq/models/DeepFM.py:86:0
  %101 : Double(100000:8, 8:1) = onnx::Cast[to=11](%100)
  %102 : Double(100000:8, 8:1) = onnx::Div(%101, %95) # /data1/xuwen/code/deepfm_v32_seq/models/DeepFM.py:87:0
  %103 : Double(100000:8, 1:8, 8:1) = onnx::Unsqueeze[axes=[1]](%102) # /data1/xuwen/code/deepfm_v32_seq/models/DeepFM.py:88:0
  %104 : Float(100000:8, 1:8, 8:1) = onnx::Cast[to=1](%103) # /data1/xuwen/code/deepfm_v32_seq/models/DeepFM.py:88:0
  %105 : Float(100000:56, 7:8, 8:1) = onnx::Concat[axis=1](%60, %65, %70, %75, %80, %92, %104) # /data1/xuwen/code/deepfm_v32_seq/models/DeepFM.py:90:0
  %106 : Long(27:1) = onnx::Constant[value=<Tensor>]()
  %107 : Double(100000:27, 27:1) = onnx::Gather[axis=1](%input, %106) # /data1/xuwen/code/deepfm_v32_seq/models/DeepFM.py:91:0
  %108 : Float(100000:27, 27:1) = onnx::Cast[to=1](%107) # /data1/xuwen/code/deepfm_v32_seq/models/DeepFM.py:91:0
  %109 : Float(100000:8, 1:8, 8:1) = onnx::ReduceSum[axes=[1], keepdims=1](%105) # /data1/xuwen/code/deepfm_v32_seq/models/DeepFM.py:64:0
  %112 : Float(100000:8, 1:8, 8:1) = onnx::Pow(%109, %177) # /data1/xuwen/code/deepfm_v32_seq/models/DeepFM.py:64:0
  %113 : Float(100000:56, 7:8, 8:1) = onnx::Mul(%105, %105) # /data1/xuwen/code/deepfm_v32_seq/models/DeepFM.py:65:0
  %114 : Float(100000:8, 1:8, 8:1) = onnx::ReduceSum[axes=[1], keepdims=1](%113) # /data1/xuwen/code/deepfm_v32_seq/models/DeepFM.py:65:0
  %115 : Float(100000:8, 1:8, 8:1) = onnx::Sub(%112, %114) # /data1/xuwen/code/deepfm_v32_seq/models/DeepFM.py:66:0
  %116 : Float(100000:1, 1:1) = onnx::ReduceSum[axes=[2], keepdims=0](%115) # /data1/xuwen/code/deepfm_v32_seq/models/DeepFM.py:67:0
  %117 : Float() = onnx::Constant[value={0.5}]()
  %118 : Float(100000:1, 1:1) = onnx::Mul(%116, %117)
  %119 : Tensor = onnx::Constant[value={0}]()
  %120 : Double(100000:54) = onnx::Gather[axis=1](%input, %119) # /data1/xuwen/code/deepfm_v32_seq/models/DeepFM.py:104:0
  %121 : Long(100000:1) = onnx::Cast[to=7](%120) # /data1/xuwen/code/deepfm_v32_seq/models/DeepFM.py:104:0
  %122 : Float(100000:1, 1:1) = onnx::Gather(%1.embed_FM_1d_sparse_col.0.weight, %121) # /usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1814:0
  %123 : Tensor = onnx::Constant[value={1}]()
  %124 : Double(100000:54) = onnx::Gather[axis=1](%input, %123) # /data1/xuwen/code/deepfm_v32_seq/models/DeepFM.py:104:0
  %125 : Long(100000:1) = onnx::Cast[to=7](%124) # /data1/xuwen/code/deepfm_v32_seq/models/DeepFM.py:104:0
  %126 : Float(100000:1, 1:1) = onnx::Gather(%1.embed_FM_1d_sparse_col.1.weight, %125) # /usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1814:0
  %127 : Tensor = onnx::Constant[value={2}]()
  %128 : Double(100000:54) = onnx::Gather[axis=1](%input, %127) # /data1/xuwen/code/deepfm_v32_seq/models/DeepFM.py:104:0
  %129 : Long(100000:1) = onnx::Cast[to=7](%128) # /data1/xuwen/code/deepfm_v32_seq/models/DeepFM.py:104:0
  %130 : Float(100000:1, 1:1) = onnx::Gather(%1.embed_FM_1d_sparse_col.2.weight, %129) # /usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1814:0
  %131 : Tensor = onnx::Constant[value={3}]()
  %132 : Double(100000:54) = onnx::Gather[axis=1](%input, %131) # /data1/xuwen/code/deepfm_v32_seq/models/DeepFM.py:104:0
  %133 : Long(100000:1) = onnx::Cast[to=7](%132) # /data1/xuwen/code/deepfm_v32_seq/models/DeepFM.py:104:0
  %134 : Float(100000:1, 1:1) = onnx::Gather(%1.embed_FM_1d_sparse_col.3.weight, %133) # /usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1814:0
  %135 : Tensor = onnx::Constant[value={4}]()
  %136 : Double(100000:54) = onnx::Gather[axis=1](%input, %135) # /data1/xuwen/code/deepfm_v32_seq/models/DeepFM.py:104:0
  %137 : Long(100000:1) = onnx::Cast[to=7](%136) # /data1/xuwen/code/deepfm_v32_seq/models/DeepFM.py:104:0
  %138 : Float(100000:1, 1:1) = onnx::Gather(%1.embed_FM_1d_sparse_col.4.weight, %137) # /usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1814:0
  %139 : Float(100000:10, 10:1, 1:1) = onnx::Gather(%1.embed_FM_1d_detail.weight, %86) # /usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1814:0
  %140 : Float(100000:1, 1:1) = onnx::ReduceSum[axes=[1], keepdims=0](%139) # /data1/xuwen/code/deepfm_v32_seq/models/DeepFM.py:106:0
  %141 : Double(100000:1, 1:1) = onnx::Cast[to=11](%140)
  %142 : Double(100000:1, 1:1) = onnx::Div(%141, %83) # /data1/xuwen/code/deepfm_v32_seq/models/DeepFM.py:107:0
  %143 : Float(100000:1, 1:1) = onnx::Cast[to=1](%142) # /data1/xuwen/code/deepfm_v32_seq/models/DeepFM.py:108:0
  %144 : Float(100000:10, 10:1, 1:1) = onnx::Gather(%1.embed_FM_1d_addf.weight, %98) # /usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1814:0
  %145 : Float(100000:1, 1:1) = onnx::ReduceSum[axes=[1], keepdims=0](%144) # /data1/xuwen/code/deepfm_v32_seq/models/DeepFM.py:110:0
  %146 : Double(100000:1, 1:1) = onnx::Cast[to=11](%145)
  %147 : Double(100000:1, 1:1) = onnx::Div(%146, %83) # /data1/xuwen/code/deepfm_v32_seq/models/DeepFM.py:111:0
  %148 : Float(100000:1, 1:1) = onnx::Cast[to=1](%147) # /data1/xuwen/code/deepfm_v32_seq/models/DeepFM.py:112:0
  %149 : Float(100000:7, 7:1) = onnx::Concat[axis=1](%122, %126, %130, %134, %138, %143, %148) # /data1/xuwen/code/deepfm_v32_seq/models/DeepFM.py:114:0
  %150 : Float(100000:1) = onnx::ReduceSum[axes=[1], keepdims=0](%149) # /data1/xuwen/code/deepfm_v32_seq/models/DeepFM.py:115:0
  %151 : Float(100000:1, 1:1) = onnx::Unsqueeze[axes=[1]](%150) # /data1/xuwen/code/deepfm_v32_seq/models/DeepFM.py:115:0
  %152 : Float(100000:1, 1:1) = onnx::MatMul(%108, %1.fm_linear.weight) # /data1/xuwen/code/deepfm_v32_seq/models/DeepFM.py:151:0
  %153 : Float(100000:1, 1:1) = onnx::Add(%118, %152) # /data1/xuwen/code/deepfm_v32_seq/models/DeepFM.py:118:0
  %154 : Float(100000:1, 1:1) = onnx::Add(%153, %151) # /data1/xuwen/code/deepfm_v32_seq/models/DeepFM.py:118:0
  %155 : Float(100000:1, 1:1) = onnx::BatchNormalization[epsilon=1.0000000000000001e-05, momentum=0.90000000000000002](%154, %1.fm_bn.weight, %1.fm_bn.bias, %1.fm_bn.running_mean, %1.fm_bn.running_var) # /usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:2014:0
  %157 : Tensor = onnx::Unsqueeze[axes=[0]](%55)
  %159 : Tensor = onnx::Concat[axis=0](%157, %178)
  %160 : Float(100000:56, 56:1) = onnx::Reshape(%105, %159) # /data1/xuwen/code/deepfm_v32_seq/models/DeepFM.py:124:0
  %161 : Float(100000:83, 83:1) = onnx::Concat[axis=1](%160, %108) # /data1/xuwen/code/deepfm_v32_seq/models/DeepFM.py:127:0
  %162 : Float(100000:128, 128:1) = onnx::Gemm[alpha=1., beta=1., transB=1](%161, %1.dnn.linear_layers.0.weight, %1.dnn.linear_layers.0.bias) # /usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1674:0
  %163 : Float(100000:128, 128:1) = onnx::BatchNormalization[epsilon=1.0000000000000001e-05, momentum=0.90000000000000002](%162, %1.dnn.bn.0.weight, %1.dnn.bn.0.bias, %1.dnn.bn.0.running_mean, %1.dnn.bn.0.running_var) # /usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:2014:0
  %164 : Float(100000:128, 128:1) = onnx::Relu(%163) # /usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:973:0
  %165 : Float(100000:64, 64:1) = onnx::Gemm[alpha=1., beta=1., transB=1](%164, %1.dnn.linear_layers.1.weight, %1.dnn.linear_layers.1.bias) # /usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1674:0
  %166 : Float(100000:64, 64:1) = onnx::BatchNormalization[epsilon=1.0000000000000001e-05, momentum=0.90000000000000002](%165, %1.dnn.bn.1.weight, %1.dnn.bn.1.bias, %1.dnn.bn.1.running_mean, %1.dnn.bn.1.running_var) # /usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:2014:0
  %167 : Float(100000:64, 64:1) = onnx::Relu(%166) # /usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:973:0
  %168 : Float(100000:32, 32:1) = onnx::Gemm[alpha=1., beta=1., transB=1](%167, %1.dnn.linear_layers.2.weight, %1.dnn.linear_layers.2.bias) # /usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1674:0
  %169 : Float(100000:32, 32:1) = onnx::BatchNormalization[epsilon=1.0000000000000001e-05, momentum=0.90000000000000002](%168, %1.dnn.bn.2.weight, %1.dnn.bn.2.bias, %1.dnn.bn.2.running_mean, %1.dnn.bn.2.running_var) # /usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:2014:0
  %170 : Float(100000:32, 32:1) = onnx::Relu(%169) # /usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:973:0
  %171 : Float(100000:16, 16:1) = onnx::Gemm[alpha=1., beta=1., transB=1](%170, %1.dnn.linear_layers.3.weight, %1.dnn.linear_layers.3.bias) # /usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1674:0
  %172 : Float(100000:16, 16:1) = onnx::BatchNormalization[epsilon=1.0000000000000001e-05, momentum=0.90000000000000002](%171, %1.dnn.bn.3.weight, %1.dnn.bn.3.bias, %1.dnn.bn.3.running_mean, %1.dnn.bn.3.running_var) # /usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:2014:0
  %173 : Float(100000:16, 16:1) = onnx::Relu(%172) # /usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:973:0
  %174 : Float(100000:1, 1:1) = onnx::Gemm[alpha=1., beta=1., transB=1](%173, %1.dnn_linear.weight, %1.dnn_linear.bias) # /usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1674:0
  %175 : Float(100000:1, 1:1) = onnx::Add(%155, %174) # /data1/xuwen/code/deepfm_v32_seq/models/DeepFM.py:137:0
  %score : Float(100000:1, 1:1) = onnx::Sigmoid(%175) # /data1/xuwen/code/deepfm_v32_seq/models/DeepFM.py:138:0
  return (%score)

